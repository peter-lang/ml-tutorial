{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Backpropagation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter-lang/ml-tutorial/blob/master/02_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQjAgOqu_jKT",
        "colab_type": "text"
      },
      "source": [
        "### Training sample: $(\\mathbf{X}, \\mathbf{D})$\n",
        "\n",
        "### Net parameters: $\\theta$\n",
        "\n",
        "$\\mathbf{Y} = \\mathit{Net}(\\mathbf{X}, \\theta)$\n",
        "\n",
        "$\\mathbf{\\hat{Y}} = \\mathit{Loss}(\\mathbf{Y}, \\mathbf{D})$\n",
        "\n",
        "### Optimization task: $$ \\underset{\\theta}{\\arg\\min}(\\mathbf{\\hat{Y}}) $$\n",
        "\n",
        "### Gradient Descent: $$ \\theta_{n+1} = \\theta_{n} - \\alpha \\nabla{\\mathbf{\\hat{Y}}} $$\n",
        "\n",
        "### $$ w_{n+1}^{i} = w_n^{i} - \\alpha \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{X}, \\theta), \\mathbf{D})}{\\partial w_n^{i}} $$\n",
        "\n",
        "<img width=\"400px\" src=\"https://miro.medium.com/max/2010/1*_6TVU8yGpXNYDkkpOfnJ6Q.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZtYhoaEG0vy",
        "colab_type": "text"
      },
      "source": [
        "### How to compute: $$ w_{n+1}^{i} = w_n^{i} - \\alpha \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{X}, \\theta), \\mathbf{D})}{\\partial w_n^{i}} $$\n",
        "\n",
        "### Partial Derivative $$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x} $$\n",
        "\n",
        "<img src=\"http://bit.ly/2ldH0Bg\">\n",
        "\n",
        "#### Linear layer: $$ \\mathit{Linear}(\\mathbf{x}) = \\sum_j{w_j x_j + b} $$\n",
        "#### $$ \\frac{\\partial \\mathit{Linear}(\\mathbf{x})}{\\partial w_i} = \\frac{\\partial \\sum_j{w_j x_j + b}}{\\partial x_i} = w_i $$\n",
        "\n",
        "#### Activation function: $$\\mathit{f}(x) = \\frac{1}{1+e^x}$$\n",
        "#### $$ \\frac{\\partial f}{\\partial x} = \\mathit{f}(x)(1-\\mathit{f}(x)) $$\n",
        "\n",
        "#### Loss function: $$ \\mathit{Loss}(y, d) = \\frac{1}{2} (y-d)^2 $$\n",
        "#### $$ \\frac{\\partial \\mathit{Loss}(y, d)}{\\partial y} = \\frac{1}{2} 2 (y-d) = y - d $$\n",
        "\n",
        "#### Compound function: $$ \\frac{\\partial \\mathit{Loss}(f(\\mathit{Linear}(\\mathbf{x})))}{\\partial w_i} = \\underbrace{\\frac{\\partial \\mathit{Loss}(f(\\mathit{Linear}(\\mathbf{x}))}{\\partial f(\\mathit{Linear}(\\mathbf{x}))}}_{f(\\mathit{Linear}(\\mathbf{x})) - d} \n",
        "\\underbrace{\\frac{\\partial f(\\mathit{Linear}(\\mathbf{x}))}{\\partial \\mathit{Linear}(\\mathbf{x})}}_{f(\\mathit{Linear}(\\mathbf{x}))(1-f(\\mathit{Linear}(\\mathbf{x})))} \n",
        "\\underbrace{\\frac{\\partial \\mathit{Linear}(\\mathbf{x})}{\\partial w_i}}_{w_i}$$\n",
        "\n",
        "#### BUT! $ f(\\mathit{Linear}(\\mathbf{x})) = \\mathit{Net}(\\mathbf{x}, \\theta) = y $ was already computed!\n",
        "\n",
        "#### $$ \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{x}, \\theta), d)}{\\partial w_i} = (y - d)y(1-y)w_i $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBEnyQRG0gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TsEVjUkMg82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  def __init__(self, in_size = None, out_size = None, next_layer = None):\n",
        "    self.next_layer = next_layer\n",
        "    \n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, in_size = None, out_size = None):\n",
        "    self.weights = weights\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return np.dot(self.weights, x)\n",
        "  \n",
        "  def df(self, x):\n",
        "    return self.weights\n",
        "  \n",
        "class Sigmoid:\n",
        "  def __init__(self, in_size = None, out_size = None, next_layer = None):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  def df(self, x):\n",
        "    return self.forward(x)*(1-self.forward(x))\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUHphE2KPkp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e4dca1c4-94aa-41f6-cc41-ec269dd89d99"
      },
      "source": [
        "Neuron(np.array([2, 3, 4]).forward(np.array([1, 1, 2]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-dfda142f1632>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Neuron(np.array([2, 3, 4]).forward(np.array([1, 1, 2]))\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    }
  ]
}