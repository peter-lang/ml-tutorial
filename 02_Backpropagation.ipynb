{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter-lang/ml-tutorial/blob/master/02_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLJLy6ImyBJa",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Basics\n",
        "\n",
        "### Number of neurons in human brain $ \\approx 10^{11} $ (100 billion)\n",
        "### 1 Nueron connects to $ \\approx 10^4 $(10 thousand)\n",
        "### Number of neural connections: $ \\approx 10^{15} $ (1 thousand-trillion)\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png\">\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1KDtsK2TVlA3DYRgCoynm0FUE1C8ivK-C\">\n",
        "\n",
        "\n",
        "| Species    | Number of synapses  | Memory (float32) |\n",
        "|------------|---------------------| ---------------- |\n",
        "| RoundWorm (fonalf√©reg) |  $ 10^{5} $ | 400 KB |\n",
        "| FruitFly (muslica) |  $ 10^{7} $ | 40 MB |\n",
        "| NeuralNets |  $ 10^{5}-10^{9} $ | 400 KB - 4 GB |\n",
        "| Bee |  $ 10^{9} $ | 4 GB |\n",
        "| Mouse |  $ 10^{12} $ | 4 TB |\n",
        "| Cat |  $ 10^{13} $ | 40 TB|\n",
        "| Human |  $ 10^{15} $ | 4 EB |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQjAgOqu_jKT",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent\n",
        "\n",
        "### Training sample: $(\\mathbf{X}, \\mathbf{D})$\n",
        "\n",
        "### Net parameters: $\\theta$\n",
        "\n",
        "$\\mathbf{Y} = \\mathit{Net}(\\mathbf{X}, \\theta)$\n",
        "\n",
        "$\\mathbf{\\hat{Y}} = \\mathit{Loss}(\\mathbf{Y}, \\mathbf{D})$\n",
        "\n",
        "### Optimization task: $$ \\underset{\\theta}{\\arg\\min}(\\mathbf{\\hat{Y}}) $$\n",
        "\n",
        "### Gradient Descent: $$ \\theta_{n+1} = \\theta_{n} - \\alpha \\nabla{\\mathbf{\\hat{Y}}} $$\n",
        "\n",
        "### $$ w_{n+1}^{i} = w_n^{i} - \\alpha \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{X}, \\theta), \\mathbf{D})}{\\partial w_n^{i}} $$\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1n1C6lTtA0vl1dryLGauTWfMGPuVta6Rt\">\n",
        "\n",
        "### Rule of thumb\n",
        "\n",
        "__Big steps__: jumps over local minima, but might diverge\n",
        "\n",
        "__Small steps__: converge, but can stuck in local minima\n",
        "\n",
        "__Learning rate schedule__:\n",
        "  - Start with small steps, so learning will converge\n",
        "  - __Warmup phase__: Gradually bigger steps, to walk through landscape and jump through local minima\n",
        "  - After we find a \"nice\" place, gradually smaller steps, to find local minima in the \"neighbourhood\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZtYhoaEG0vy",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent At Final Layer\n",
        "\n",
        "### How to compute in general: $$ w_{n+1}^{i} = w_n^{i} - \\alpha \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{X}, \\theta), \\mathbf{D})}{\\partial w_n^{i}} $$\n",
        "\n",
        "### Partial Derivative $$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x} $$\n",
        "\n",
        "### Final layer\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=13-rotnmfvdl3kq8DxbtSSzyQbYN1GTOj\">\n",
        "\n",
        "\n",
        "#### Linear layer: $$ \\mathit{Linear}(\\mathbf{x}) = \\sum_j{w_j x_j} $$\n",
        "#### $$ \\frac{\\partial \\mathit{Linear}(\\mathbf{x})}{\\partial w_i} = \\frac{\\partial \\sum_j{w_j x_j}}{\\partial x_i} = x_i $$\n",
        "#### $$ \\frac{\\partial \\mathit{Linear}(\\mathbf{x})}{\\partial w_n } = \\frac{\\partial \\sum_j{w_j x_j}}{\\partial w_n} = x_n = 1 $$\n",
        "\n",
        "#### Activation function: $$\\varphi(x) = \\frac{1}{1+e^{-x}}$$\n",
        "#### $$ \\frac{\\partial \\varphi}{\\partial x} = \\varphi(x)(1-\\varphi(x)) $$\n",
        "\n",
        "#### Loss function: $$ \\mathit{Loss}(y, d) = \\frac{1}{2} (y-d)^2 $$\n",
        "#### $$ \\frac{\\partial \\mathit{Loss}(y, d)}{\\partial y} = \\frac{1}{2} 2 (y-d) = y - d $$\n",
        "\n",
        "#### Compound function: $$ \\frac{\\partial \\mathit{Loss}(\\varphi(\\mathit{Linear}(\\mathbf{x})))}{\\partial w_i} = \\underbrace{\\frac{\\partial \\mathit{Loss}(\\varphi(\\mathit{Linear}(\\mathbf{x}))}{\\partial \\varphi(\\mathit{Linear}(\\mathbf{x}))}}_{\\varphi(\\mathit{Linear}(\\mathbf{x})) - d} \n",
        "\\underbrace{\\frac{\\partial \\varphi(\\mathit{Linear}(\\mathbf{x}))}{\\partial \\mathit{Linear}(\\mathbf{x})}}_{\\varphi(\\mathit{Linear}(\\mathbf{x}))(1-\\varphi(\\mathit{Linear}(\\mathbf{x})))} \n",
        "\\underbrace{\\frac{\\partial \\mathit{Linear}(\\mathbf{x})}{\\partial w_i}}_{x_i}$$\n",
        "\n",
        "#### BUT! $ \\varphi(\\mathit{Linear}(\\mathbf{x})) = \\mathit{Net}(\\mathbf{x}, \\theta) = y $ was already computed during __FORWARD PATH__\n",
        "\n",
        "#### $$ \\frac{\\partial \\mathit{Loss}(\\mathit{Net}(\\mathbf{x}, \\theta), d)}{\\partial w_i} = (y - d)y(1-y)x_i $$\n",
        "\n",
        "#### Let's also save this for later usage, also known as __BACKWARD PATH__ :)\n",
        "\n",
        "#### $$ \\delta =  \\frac{\\partial \\mathit{Loss}(y, d)}{\\partial \\mathit{Linear}(\\mathbf{x})} = (y - d)y(1-y)$$\n",
        "\n",
        "\n",
        "# Gradient Descent at any place\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yNyUDqz4pqNDSWurzgRuUp56SaFv8V2l\">\n",
        "\n",
        "\n",
        "## Lessons learnt\n",
        "- __FORWARD PATH__:\n",
        "  - Needed to compute $ Loss(y, d) $ function\n",
        "  - Save partial results at each neuron output: $ y_i $ \n",
        "- __BACKWARD PATH__: \n",
        "  - To compute gradients with regards to every weight\n",
        "  - Save partial derivative results at each neuron input: $ \\delta_i $\n",
        "- __Neural Nets & Backpropagation algorithm__: \n",
        "  - Given $ N $ weights (a.k.a parameters)\n",
        "  - Steps: $ 2N $\n",
        "    - Forward\n",
        "    - Backward\n",
        "  - Memory: $ 3N $\n",
        "    - Original weight\n",
        "    - Forward partial result\n",
        "    - Backward partial result\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52W4KfyLqNK",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent at any place\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBEnyQRG0gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TsEVjUkMg82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  def __init__(self, in_size = None, out_size = None, next_layer = None):\n",
        "    self.next_layer = next_layer\n",
        "    \n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, in_size = None, out_size = None):\n",
        "    self.weights = weights\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return np.dot(self.weights, x)\n",
        "  \n",
        "  def df(self, x):\n",
        "    return self.weights\n",
        "  \n",
        "class Sigmoid:\n",
        "  def __init__(self, in_size = None, out_size = None, next_layer = None):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  def df(self, x):\n",
        "    return self.forward(x)*(1-self.forward(x))\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUHphE2KPkp5",
        "colab_type": "code",
        "outputId": "e4dca1c4-94aa-41f6-cc41-ec269dd89d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "Neuron(np.array([2, 3, 4]).forward(np.array([1, 1, 2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-dfda142f1632>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Neuron(np.array([2, 3, 4]).forward(np.array([1, 1, 2]))\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    }
  ]
}